global_overrides:
  per_tenant_override_config: /conf/overrides.yaml
  metrics_generator_processors:
    - service-graphs
    - span-metrics

ingester:
  replicas: 1
  autoscaling:
    # -- Enable autoscaling for the ingester
    enabled: false
    # -- Minimum autoscaling replicas for the ingester
    minReplicas: 1
    # -- Maximum autoscaling replicas for the ingester
    maxReplicas: 3
    # -- Target CPU utilisation percentage for the ingester
    targetCPUUtilizationPercentage: 80
    # -- Target memory utilisation percentage for the ingester
    targetMemoryUtilizationPercentage: 80

  resources:
    requests:
      cpu: 150m
      memory: 200Mi
    limits:
      cpu: 300m
      memory: 400Mi
  
  persistence:
    # -- Enable creating PVCs which is required when using boltdb-shipper
    enabled: true
    # -- use emptyDir with ramdisk instead of PVC. **Please note that all data in ingester will be lost on pod restart**
    inMemory: false
    # -- Size of persistent or memory disk
    size: 2Gi

  config:
    replication_factor: 1 # must be > 0


# Configuration for the distributor
distributor:
  # -- Number of replicas for the distributor
  replicas: 1
  autoscaling:
    # -- Enable autoscaling for the distributor
    enabled: false
    # -- Minimum autoscaling replicas for the distributor
    minReplicas: 1
    # -- Maximum autoscaling replicas for the distributor
    maxReplicas: 3
    # -- Target CPU utilisation percentage for the distributor
    targetCPUUtilizationPercentage: 80
    # -- Target memory utilisation percentage for the distributor
    targetMemoryUtilizationPercentage: 80

  resources:
    requests:
      cpu: 150m
      memory: 200Mi
    limits:
      cpu: 300m
      memory: 400Mi
  affinity: null

  config:
    log_received_traces: false
    log_received_spans:
      enabled: false

compactor:
  # -- Number of replicas for the compactor
  replicas: 1
  resources:
    requests:
      cpu: 150m
      memory: 200Mi
    limits:
      cpu: 300m
      memory: 400Mi

  config:
    compaction:
      # -- Duration to keep blocks
      block_retention: 48h


# Configuration for the querier
querier:
  # -- Number of replicas for the querier
  replicas: 1

  resources:
    requests:
      cpu: 150m
      memory: 200Mi
    limits:
      cpu: 300m
      memory: 400Mi
  config:
    query_timeout: 30s
    #frontend_worker:
      #frontend_address: tempo-distributed-query-frontend:9095
      #grpc_client_config: {}


search:
  # -- Enable Tempo search
  enabled: true


traces:
  otlp:
    http:
      # -- Enable Tempo to ingest Open Telemetry HTTP traces
      enabled: true
      # -- HTTP receiver advanced config
      receiverConfig: {}
    grpc:
      # -- Enable Tempo to ingest Open Telemetry GRPC traces
      enabled: true
      # -- GRPC receiver advanced config
      receiverConfig: {}


# Set Tempo server configuration
# Refers to https://grafana.com/docs/tempo/latest/configuration/#server
server:
  # --  HTTP server listen host
  httpListenPort: 3100
  # -- Log level. Can be set to trace, debug, info (default), warn error, fatal, panic
  logLevel: info
  # -- Log format. Can be set to logfmt (default) or json.
  logFormat: logfmt
  # -- Max gRPC message size that can be received
  grpc_server_max_recv_msg_size: 4194304
  # -- Max gRPC message size that can be sent
  grpc_server_max_send_msg_size: 4194304



# NOTE: In its default configuration, the chart uses local filesystem as storage. The reason for this is that the chart can be validated and installed in a CI pipeline. However, this setup is not fully functional. The recommendation is to use object storage, such as S3, GCS, MinIO, etc., or one of the other options documented at https://grafana.com/docs/tempo/latest/configuration/#storage.
storage:
  trace:
    # -- The supported storage backends are gcs, s3 and azure, as specified in https://grafana.com/docs/tempo/latest/configuration/#storage
    backend: local

# memcached is for all of the Tempo pieces to coordinate with each other.
# you can use your self memcacherd by set enable: false and host + service
memcached:
  # -- Specified whether the memcached cachce should be enabled
  enabled: true
  # Number of replicas for memchached
  replicas: 1
  resources:
    requests:
      cpu: 150m
      memory: 200Mi
    limits:
      cpu: 300m
      memory: 400Mi


# Configuration for the metrics-generator
metricsGenerator:
  # -- Specifies whether a metrics-generator should be deployed
  enabled: true
  # -- Number of replicas for the metrics-generator
  replicas: 1
  resources:
    requests:
      cpu: 150m
      memory: 200Mi
    limits:
      cpu: 300m
      memory: 400Mi
  affinity: null
  config:
    #  MaxItems is the amount of edges that will be stored in the store.
    service_graphs_max_items: 10000
    storage_remote_write:
      - url: http://prometheus-server.prometheus:80/api/v1/write
        send_exemplars: true
        headers:
          x-scope-orgid: operations


tempo: 
  structuredConfig:
    query_frontend:
      search: 
        max_duration: "72h"
        #default_result_limit: 30
        #max_result_limit: 150
        #query_backend_after: 10000h
        #query_ingesters_until: 10000h

 